{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --q git+https://github.com/m-bain/whisperx.git openai-whisper pydub langchain openai==0.27.2 faiss-cpu tiktoken sentence-transformers googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrTWWJYg89_Z",
        "outputId": "e72aa4b5-1361-42d8-84d1-3e399df5acd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eslu18qN80bT",
        "outputId": "d79c6c9a-38cd-478c-a0e7-54860c5e9d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Importing Libraries\n",
        "import whisper\n",
        "import whisperx\n",
        "import csv\n",
        "import gc\n",
        "from googletrans import Translator\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from typing_extensions import Concatenate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "import heapq\n",
        "import urllib.request\n",
        "import re\n",
        "from moviepy.editor import AudioFileClip\n",
        "import pandas as pd\n",
        "import csv\n",
        "import openai\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Accepting for Prefered Language\n",
        "def language(choice):\n",
        "  if choice == 1:\n",
        "      source_lang = 'hi'\n",
        "  elif choice == 2:\n",
        "      source_lang = 'bn'\n",
        "  elif choice == 3:\n",
        "      source_lang = 'te'\n",
        "  elif choice == 4:\n",
        "      source_lang = 'mr'\n",
        "  elif choice == 5:\n",
        "      source_lang = 'ta'\n",
        "  elif choice == 6:\n",
        "      source_lang = 'ur'\n",
        "  elif choice == 7:\n",
        "      source_lang = 'gu'\n",
        "  elif choice == 8:\n",
        "      source_lang = 'kn'\n",
        "  elif choice == 9:\n",
        "      source_lang = 'ml'\n",
        "  elif choice == 10:\n",
        "      source_lang = 'en'\n",
        "  return source_lang\n",
        "\n",
        "# Code for Converting MP4 format to MP3 format\n",
        "def mp4tomp3(video_file_path, mp3_file_path):\n",
        "  command_to_mp3 = f\"ffmpeg -i {video_file_path} {mp3_file_path}\"\n",
        "  os.system(command_to_mp3)\n",
        "\n",
        "# Code for Transcription in Mutliple languages\n",
        "def translate_transcript(text, target_lang='en'):\n",
        "    translator = Translator()\n",
        "    translated_text = \"\"\n",
        "    segments = text.split()\n",
        "    prev_lang = None\n",
        "    current_segment = \"\"\n",
        "    for segment in segments:\n",
        "        detected_lang = translator.detect(segment).lang\n",
        "        if detected_lang != prev_lang:\n",
        "            if prev_lang:\n",
        "                if prev_lang != 'en':\n",
        "                    translated_segment = translator.translate(current_segment, src=prev_lang, dest=target_lang)\n",
        "                    translated_text += translated_segment.text.strip() + \" \"\n",
        "                else:\n",
        "                    translated_text += current_segment.strip() + \" \"\n",
        "                current_segment = \"\"\n",
        "            prev_lang = detected_lang\n",
        "        current_segment += segment + \" \"\n",
        "    if prev_lang and prev_lang != 'en':\n",
        "        translated_segment = translator.translate(current_segment.strip(), src=prev_lang, dest=target_lang)\n",
        "        translated_text += translated_segment.text.strip()\n",
        "    else:\n",
        "        translated_text += current_segment.strip()\n",
        "    return translated_text.strip()\n",
        "\n",
        "def transcriptnormal(file_path, filename):\n",
        "    model = whisper.load_model(\"small\")\n",
        "    result = model.transcribe(file_path)\n",
        "    transcript = result[\"text\"]\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "      file.write(transcript)\n",
        "    translated_transcript=translate_transcript(transcript)\n",
        "    return translated_transcript\n",
        "\n",
        "# Code for TimeStamp Function\n",
        "def translate_text(text, target_lang='en'):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, dest=target_lang).text\n",
        "    return translated_text\n",
        "\n",
        "def transcriptcsv(audio_file, csv_filename):\n",
        "  device = \"cuda\"\n",
        "  batch_size = 16\n",
        "  compute_type = \"float16\"\n",
        "  model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "  audio = whisperx.load_audio(audio_file)\n",
        "  result = model.transcribe(audio, batch_size=batch_size)\n",
        "  with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['text', 'start', 'end']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for segment in result['segments']:\n",
        "            writer.writerow(segment)\n",
        "  df = pd.read_csv(csv_filename)\n",
        "  output_csv_file_name = f\"{name}translated_file.csv\"\n",
        "  for index, row in df.iterrows():\n",
        "      translated_row = row.apply(lambda x: translate_text(str(x)))\n",
        "      df.loc[index] = translated_row\n",
        "  df.to_csv(output_csv_file_name, index=False)\n",
        "\n",
        "# Code for Minutes of Meeting Preparation\n",
        "def translateentolang(text, target_lang):\n",
        "  source_lang='en'\n",
        "  translator = Translator()\n",
        "  translated_text = translator.translate(text, src=source_lang, dest=target_lang)\n",
        "  return translated_text.text\n",
        "\n",
        "def save_to_txt(txt_path, title, summary_text, agenda, tasks, important):\n",
        "    with open(txt_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"Title:\\n\" + title + \"\\n\\n\")\n",
        "        file.write(\"Agenda:\\n\" + agenda + \"\\n\\n\")\n",
        "        file.write(\"Summary:\\n\" + summary_text + \"\\n\\n\")\n",
        "        file.write(\"Tasks:\\n\" + tasks + \"\\n\\n\")\n",
        "        file.write(\"Important:\\n\" + important + \"\\n\\n\")\n",
        "\n",
        "def MOMAnswer(query, document_search, chain):\n",
        "    docs = document_search.similarity_search(query)\n",
        "    return chain.run(input_documents=docs, question=query)\n",
        "\n",
        "def MOM(file_path, Language, txt_path, text_meet, attendence_file_path):\n",
        "  content_text = re.sub(r'\\[[0-9]*\\]', ' ', text_meet)\n",
        "  content_text = re.sub(r'\\s+', ' ', content_text)\n",
        "  formatted_content_text = re.sub('[^a-zA-Z]', ' ', content_text )\n",
        "  formatted_content_text = re.sub(r'\\s+', ' ', formatted_content_text)\n",
        "\n",
        "  sentence_list = nltk.sent_tokenize(content_text)\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  word_frequencies = {}\n",
        "  for word in nltk.word_tokenize(formatted_content_text):\n",
        "      if word not in stopwords:\n",
        "          if word not in word_frequencies.keys():\n",
        "              word_frequencies[word] = 1\n",
        "          else:\n",
        "              word_frequencies[word] += 1\n",
        "  maximum_frequncy = max(word_frequencies.values())\n",
        "  for word in word_frequencies.keys():\n",
        "      word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "\n",
        "  sentence_scores = {}\n",
        "  for sent in sentence_list:\n",
        "      for word in nltk.word_tokenize(sent.lower()):\n",
        "          if word in word_frequencies.keys():\n",
        "              if len(sent.split(' ')) < 30:\n",
        "                  if sent not in sentence_scores.keys():\n",
        "                      sentence_scores[sent] = word_frequencies[word]\n",
        "                  else:\n",
        "                      sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "  summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "  summary = ' '.join(summary_sentences)\n",
        "\n",
        "  attendee_names = []\n",
        "  with open(attendence_file_path, 'r') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      first_name = row['First name']\n",
        "      attendee_names.append(first_name)\n",
        "\n",
        "\n",
        "  os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
        "  text_splitter=CharacterTextSplitter(separator=\"\\n\", chunk_size = 100, chunk_overlap = 20,)\n",
        "  texts=text_splitter.split_text(text_meet)\n",
        "  embeddings= HuggingFaceEmbeddings()\n",
        "  document_search=FAISS.from_texts(texts,embeddings)\n",
        "  chain=load_qa_chain(OpenAI(),chain_type=\"stuff\")\n",
        "  queries = [\n",
        "      \"Provide suitable title of the meet\",\n",
        "      \"What was the agenda of the meet? Provide in points. Maximum 3 Points. After each point end with '\\n'\",\n",
        "      f\"Mention the task allocated to each attendee for future. Provide in points. These are the attendee list {attendee_names}. In this format (Attendee1) : Task1, Task2,..,Task n;\\n (Attendee2) : Task1, Task2,..,Task n;\\n ...;\\n (Attendee n) : Task1, Task2,..,Task n; where attendee name is in curve brackets. After each Each attendee and his/her tasks end with '\\n'\",\n",
        "      \"Mention the important topics discussed in the meet. Provide in points. After each point end with '\\n'\"\n",
        "  ]\n",
        "  i = 1\n",
        "  result=[]\n",
        "  while i != 5:\n",
        "      query = queries[i - 1]\n",
        "      result.append(MOMAnswer(query, document_search, chain))\n",
        "      i += 1\n",
        "  title=result[0]\n",
        "  agenda=result[1]\n",
        "  tasks=result[2]\n",
        "  important=result[3]\n",
        "\n",
        "  if Language!='en':\n",
        "    title = translateentolang(title, target_lang=Language)\n",
        "    agenda = translateentolang(agenda, target_lang=Language)\n",
        "    summary = translateentolang(summary, target_lang=Language)\n",
        "    tasks = translateentolang(tasks, target_lang=Language)\n",
        "    important = translateentolang(important, target_lang=Language)\n",
        "    save_to_txt(txt_path, title, summary, agenda, tasks, important)\n",
        "  else:\n",
        "    save_to_txt(txt_path, title, summary, agenda, tasks, important)\n"
      ],
      "metadata": {
        "id": "BcgR-Stq9faD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for accepting preffered language\n",
        "print(\"Available languages:\")\n",
        "print(\"1. Hindi\")\n",
        "print(\"2. Bengali\")\n",
        "print(\"3. Telugu\")\n",
        "print(\"4. Marathi\")\n",
        "print(\"5. Tamil\")\n",
        "print(\"6. Urdu\")\n",
        "print(\"7. Gujarati\")\n",
        "print(\"8. Kannada\")\n",
        "print(\"9. Malayalam\")\n",
        "print(\"10. English\")\n",
        "Languagechoice=int(input(\"Choose the prefered languages:\"))\n",
        "Language=language(Languagechoice)\n",
        "# Code for accepting video file path\n",
        "video_file_path=input(\"Enter the video path:\")\n",
        "attendence_file_path=input(\"Enter the attendence csv:\")\n",
        "name = video_file_path.rsplit(\".\", 1)[0]\n",
        "mp3_file_path=f\"{name}.mp3\"\n",
        "mp4tomp3(video_file_path, mp3_file_path)\n",
        "# Code for transcripting video\n",
        "filename=f\"{name}.txt\"\n",
        "text_meet=transcriptnormal(mp3_file_path, filename)\n",
        "# Code for transcripting video with timestamp\n",
        "csv_filename=f\"{name}.csv\"\n",
        "transcriptcsv(video_file_path, csv_filename)\n",
        "# Code for saving Minutes of Meeting\n",
        "txt_path=f\"{name}MOM.txt\"\n",
        "MOM(filename, Language, txt_path, text_meet, attendence_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idQ5egJz9qpu",
        "outputId": "13ac4e3b-56a3-4915-aedf-52e7bdc3a1d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available languages:\n",
            "1. Hindi\n",
            "2. Bengali\n",
            "3. Telugu\n",
            "4. Marathi\n",
            "5. Tamil\n",
            "6. Urdu\n",
            "7. Gujarati\n",
            "8. Kannada\n",
            "9. Malayalam\n",
            "10. English\n",
            "Choose the prefered languages:1\n",
            "Enter the video path:/content/drive/MyDrive/Meetings/Meeting.mp4\n",
            "Enter the attendence csv:/content/drive/MyDrive/Meetings/Attendence.csv\n",
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (0.94) in first 30s of audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.llms import OpenAI\n",
        "import openai\n",
        "\n",
        "def list_files(directory):\n",
        "    files = os.listdir(directory)\n",
        "    return f\"Files and directories in {directory}:\\n\" + \"\\n\".join(files)\n",
        "\n",
        "def change_directory(path):\n",
        "    os.chdir(path)\n",
        "    return f\"Current directory changed to {os.getcwd()}\"\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "    return f\"Contents of {file_path}:\\n{content}\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"List Files\",\n",
        "        func=list_files,\n",
        "        description=\"List all files and directories in a given directory\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Change Directory\",\n",
        "        func=change_directory,\n",
        "        description=\"Change the current working directory\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Read File\",\n",
        "        func=read_file,\n",
        "        description=\"Read the contents of a file\"\n",
        "    )\n",
        "]\n",
        "toolsgreet=[]\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=\"conversational-react-description\", verbose=True, handle_parsing_errors=True)\n",
        "agentgreet = initialize_agent(toolsgreet, llm, agent=\"conversational-react-description\", verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "User_Name=input(\"Enter your Name: \")\n",
        "Initial_Path=\"/content/drive/MyDrive/Meetings\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "Greetings = agentgreet.run(\n",
        "    {\n",
        "        \"input\": f\"Greet the user {User_Name} and ask how you might assist him/her today\",\n",
        "        \"chat_history\": \"your name is Astute and you are assisting bot\"\n",
        "    }\n",
        ")\n",
        "print(Greetings)\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def store_chat_history(input_text, output_text):\n",
        "    chat_history.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "while True:\n",
        "    question = input(\"Enter the Question:\")\n",
        "    if question==\"exit\":\n",
        "      break\n",
        "    else:\n",
        "      current_file_path = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=[{\"role\": \"user\", \"content\": f\"Identify the file name in the line :{question}. Only give filename as answer\"}],\n",
        "          max_tokens=1024,\n",
        "          n=1,\n",
        "          stop=None,\n",
        "          temperature=0.5,\n",
        "      )\n",
        "      current=current_file_path.choices[0].message.content.strip()\n",
        "      context = {\n",
        "          \"input\": f\"Directory: {Initial_Path}, File Path: {Initial_Path}/{current}, I am: {User_Name}, Answer the following question: {question}\",\n",
        "          \"chat_history\": chat_history[-4:]\n",
        "      }\n",
        "      response = agent.run(context)\n",
        "      print(\"AI:\", response)\n",
        "      store_chat_history(question, response)"
      ],
      "metadata": {
        "id": "TUzCHAPy_WFV",
        "outputId": "ef6581cd-917e-4ded-c51a-e7c96bf644e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Name: Aditya Purwar\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: Hello Aditya Purwar, how can I assist you today?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello Aditya Purwar, how can I assist you today?\n",
            "Enter the Question:Can you please tell my work discussed in Meeting.txt\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Read File\n",
            "Action Input: /content/drive/MyDrive/Meetings/Meeting.txt\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mContents of /content/drive/MyDrive/Meetings/Meeting.txt:\n",
            " Hello, good afternoon, Radhe Verma and Aditya Shree Gupta. I am Aditya Purva and today we have called this meet to discuss upon the upcoming project, which is Astute. And as we know that we have to complete it by 27th of March. So it is basically we have to create a meeting software where users can interact and schedule their all the tasks by the help of the recorded video provided to them. And the MOM, which is minutes of meeting can be stored. So first thing what plan I have is about transcripting it. So as I know Aditya Shree, you have worked on it. Can you please elaborate how can we implement on our project? Yeah, actually we have many libraries and modules in Python such as MoviePy and Speech Recognition. And we also have OpenAI Whisper. But I use OpenAI Whisper because MoviePy and Speech Recognition only provided transcript in English. Whereas OpenAI Whisper provided us with a multi-linger transcription, which I feel is the most useful regarding the diverse Indian market. It also has an extension WhisperX which provides a timestamp feature, which can also be further used for navigating through the transcript. Okay, cool. So we will be going with the Open Whisper. So as you are clear with it, I expect that Aditya Shree will complete that project by 13th of March. Okay, then as I will be implementing a launching framework for my generative AI model, and I expect it to be completed by 16. So, but as I know that LLM's can't provide the responses multilingualy. So this is why I wanted the responses as well in Indian languages. So, Aradhir, do you have any plan for it because I have seen your previous work on that? So please, Rav Rathak. Yeah, sure. So basically there are a few libraries which we can use. I will be comparing one of the major ones, which is Google Trans and Hugging Face Transformers. We will be preferring Google Trans because of its advantages. I'll tell the advantages. Basically, Google Trans is relatively easier to set up and use, and integrating Google Trans is relatively easier than Hugging Face Transformer library. And basically, Google Trans can perform offline translation as well. So we will not be requiring much internet, and Hugging Face Transformer typically requires more internet access. So due to all these advantages that Google Trans have, we will be preferring this library. Thank you. Okay, cool. So we will be going Google Trans status. So as I will be completing my project by 16, so I expect that you integrate my project with your multilingual model by 17th of March. And that will be completed for the phase one. And then after which we will be discussing phase two in the next meeting. So that's all. And please complete your project by the given deadline. And thank you, Akram. All the best.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: Sure, I can summarize the main points discussed in the meeting for you. The project discussed was Astute, a meeting software that allows users to interact and schedule tasks using recorded videos. The plan is to use OpenAI Whisper for multi-lingual transcription and Google Trans for multi-lingual responses. The deadline for phase one is 17th of March. Is there anything else you would like me to clarify?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Sure, I can summarize the main points discussed in the meeting for you. The project discussed was Astute, a meeting software that allows users to interact and schedule tasks using recorded videos. The plan is to use OpenAI Whisper for multi-lingual transcription and Google Trans for multi-lingual responses. The deadline for phase one is 17th of March. Is there anything else you would like me to clarify?\n",
            "Enter the Question:What is My work deadline in Meeting.txt\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Read File\n",
            "Action Input: /content/drive/MyDrive/Meetings/Meeting.txt\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mContents of /content/drive/MyDrive/Meetings/Meeting.txt:\n",
            " Hello, good afternoon, Radhe Verma and Aditya Shree Gupta. I am Aditya Purva and today we have called this meet to discuss upon the upcoming project, which is Astute. And as we know that we have to complete it by 27th of March. So it is basically we have to create a meeting software where users can interact and schedule their all the tasks by the help of the recorded video provided to them. And the MOM, which is minutes of meeting can be stored. So first thing what plan I have is about transcripting it. So as I know Aditya Shree, you have worked on it. Can you please elaborate how can we implement on our project? Yeah, actually we have many libraries and modules in Python such as MoviePy and Speech Recognition. And we also have OpenAI Whisper. But I use OpenAI Whisper because MoviePy and Speech Recognition only provided transcript in English. Whereas OpenAI Whisper provided us with a multi-linger transcription, which I feel is the most useful regarding the diverse Indian market. It also has an extension WhisperX which provides a timestamp feature, which can also be further used for navigating through the transcript. Okay, cool. So we will be going with the Open Whisper. So as you are clear with it, I expect that Aditya Shree will complete that project by 13th of March. Okay, then as I will be implementing a launching framework for my generative AI model, and I expect it to be completed by 16. So, but as I know that LLM's can't provide the responses multilingualy. So this is why I wanted the responses as well in Indian languages. So, Aradhir, do you have any plan for it because I have seen your previous work on that? So please, Rav Rathak. Yeah, sure. So basically there are a few libraries which we can use. I will be comparing one of the major ones, which is Google Trans and Hugging Face Transformers. We will be preferring Google Trans because of its advantages. I'll tell the advantages. Basically, Google Trans is relatively easier to set up and use, and integrating Google Trans is relatively easier than Hugging Face Transformer library. And basically, Google Trans can perform offline translation as well. So we will not be requiring much internet, and Hugging Face Transformer typically requires more internet access. So due to all these advantages that Google Trans have, we will be preferring this library. Thank you. Okay, cool. So we will be going Google Trans status. So as I will be completing my project by 16, so I expect that you integrate my project with your multilingual model by 17th of March. And that will be completed for the phase one. And then after which we will be discussing phase two in the next meeting. So that's all. And please complete your project by the given deadline. And thank you, Akram. All the best.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: Your work deadline in Meeting.txt is 27th of March. Is there anything else you would like me to clarify?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "AI: Your work deadline in Meeting.txt is 27th of March. Is there anything else you would like me to clarify?\n",
            "Enter the Question:exit\n"
          ]
        }
      ]
    }
  ]
}